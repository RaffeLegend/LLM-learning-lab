{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零实现 Transformer：理论到代码的完整对照\n",
    "\n",
    "## 实验目标\n",
    "\n",
    "1. **从零实现** Scaled Dot-Product Attention、Multi-Head Attention、位置编码（Sinusoidal + RoPE）、FFN（ReLU vs SwiGLU）、完整 Transformer Block\n",
    "2. **可视化验证** $\\sqrt{d_k}$ 缩放对 softmax 饱和的影响\n",
    "3. **实现 Causal Mask + KV Cache** 推理优化，对比推理速度\n",
    "4. **训练一个 Mini GPT** 做字符级语言建模（Next-Token Prediction）\n",
    "5. **消融实验**：有/无位置编码、Pre-LN vs Post-LN、ReLU vs SwiGLU\n",
    "\n",
    "## 预期结果\n",
    "\n",
    "- 手写 Attention 输出与 PyTorch 内置实现一致（数值误差 < 1e-5）\n",
    "- 无缩放时 softmax 输出趋近 one-hot（饱和），有缩放时分布更均匀\n",
    "- 训练 loss 在 ~500 步内从 ~3.5 下降到 < 1.5\n",
    "- 无位置编码的模型 loss 明显高于有位置编码的模型\n",
    "- KV Cache 推理速度快于无 Cache（序列越长加速越明显）\n",
    "\n",
    "## 所需环境\n",
    "\n",
    "- Python >= 3.9\n",
    "- PyTorch >= 2.0\n",
    "- matplotlib\n",
    "- numpy\n",
    "\n",
    "## 关联笔记\n",
    "\n",
    "- [Transformer 架构详解](../../notes/fundamentals/transformer.md)\n",
    "- [位置编码（RoPE）](../../notes/fundamentals/positional-encoding.md)\n",
    "- [大模型预训练](../../notes/training/llm-pretraining.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Part 1: 基础设置 ========\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 固定随机种子，确保实验可复现\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 自动选择设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "print(f'PyTorch 版本: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scaled Dot-Product Attention\n",
    "\n",
    "Attention 的核心公式：\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "为什么要除以 $\\sqrt{d_k}$？当 $d_k$ 较大时，$QK^T$ 的元素方差为 $d_k$（假设 Q、K 各元素独立标准正态），\n",
    "导致 softmax 输入值很大，输出趋近 one-hot（梯度接近 0，训练困难）。\n",
    "除以 $\\sqrt{d_k}$ 后方差归一化为 1，softmax 输出分布更加均匀。\n",
    "\n",
    "下面我们先实现这个函数，然后**用可视化证明缩放的必要性**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 从零实现 Scaled Dot-Product Attention ========\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    手写 Scaled Dot-Product Attention\n",
    "    \n",
    "    Args:\n",
    "        Q: (batch, seq_len_q, d_k)\n",
    "        K: (batch, seq_len_k, d_k)\n",
    "        V: (batch, seq_len_k, d_v)\n",
    "        mask: (batch, seq_len_q, seq_len_k) 或 (seq_len_q, seq_len_k)\n",
    "              True/1 的位置会被 mask（设为 -inf）\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, seq_len_q, d_v)\n",
    "        attn_weights: (batch, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: 计算 Q @ K^T，得到注意力分数\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_q, seq_k)\n",
    "    \n",
    "    # Step 2: 缩放 —— 除以 sqrt(d_k) 防止 softmax 饱和\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: 应用 mask（因果掩码或 padding 掩码）\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
    "    \n",
    "    # Step 4: softmax 归一化\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: 加权求和\n",
    "    output = torch.matmul(attn_weights, V)  # (batch, seq_q, d_v)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# ======== 验证：与 PyTorch 内置实现对比 ========\n",
    "set_seed(42)\n",
    "B, T, D = 2, 8, 16  # batch=2, seq_len=8, d_k=16\n",
    "Q = torch.randn(B, T, D)\n",
    "K = torch.randn(B, T, D)\n",
    "V = torch.randn(B, T, D)\n",
    "\n",
    "# 我们的实现\n",
    "our_output, our_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# PyTorch 内置实现（需要转换 shape: PyTorch expects (B, num_heads, T, D)）\n",
    "# 这里 num_heads=1，所以 unsqueeze\n",
    "pt_output = F.scaled_dot_product_attention(Q.unsqueeze(1), K.unsqueeze(1), V.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "max_diff = (our_output - pt_output).abs().max().item()\n",
    "print(f'与 PyTorch 内置实现的最大差异: {max_diff:.2e}')\n",
    "assert max_diff < 1e-5, '实现不一致！'\n",
    "print('✓ 验证通过：手写实现与 PyTorch 内置一致')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 可视化：sqrt(d_k) 缩放的效果 ========\n",
    "# 核心实验：展示缩放如何防止 softmax 饱和\n",
    "\n",
    "set_seed(42)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('$\\\\sqrt{d_k}$ 缩放对 softmax 分布的影响', fontsize=14)\n",
    "\n",
    "for idx, d_k in enumerate([8, 64, 512]):\n",
    "    Q = torch.randn(1, 8, d_k)\n",
    "    K = torch.randn(1, 8, d_k)\n",
    "    \n",
    "    # 不缩放\n",
    "    raw_scores = torch.matmul(Q, K.transpose(-2, -1))  # 方差 ≈ d_k\n",
    "    raw_attn = F.softmax(raw_scores, dim=-1)\n",
    "    \n",
    "    # 有缩放\n",
    "    scaled_scores = raw_scores / math.sqrt(d_k)  # 方差 ≈ 1\n",
    "    scaled_attn = F.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    # 上排：无缩放\n",
    "    im0 = axes[0, idx].imshow(raw_attn[0].detach().numpy(), vmin=0, vmax=1, cmap='Blues')\n",
    "    axes[0, idx].set_title(f'无缩放, $d_k$={d_k}\\n分数方差={raw_scores.var():.1f}')\n",
    "    axes[0, idx].set_xlabel('Key 位置')\n",
    "    axes[0, idx].set_ylabel('Query 位置')\n",
    "    \n",
    "    # 下排：有缩放\n",
    "    im1 = axes[1, idx].imshow(scaled_attn[0].detach().numpy(), vmin=0, vmax=1, cmap='Blues')\n",
    "    axes[1, idx].set_title(f'有缩放, $d_k$={d_k}\\n分数方差={scaled_scores.var():.1f}')\n",
    "    axes[1, idx].set_xlabel('Key 位置')\n",
    "    axes[1, idx].set_ylabel('Query 位置')\n",
    "\n",
    "plt.colorbar(im1, ax=axes, shrink=0.6, label='Attention 权重')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('观察：')\n",
    "print('- 无缩放 + 大 d_k 时，softmax 输出趋近 one-hot（颜色集中在单个位置）')\n",
    "print('- 有缩放后，注意力权重分布更均匀，梯度不会消失')\n",
    "print(f'- d_k=512 无缩放时，最大注意力权重 ≈ {raw_attn[0].max():.4f}（接近 1.0）')\n",
    "print(f'- d_k=512 有缩放时，最大注意力权重 ≈ {scaled_attn[0].max():.4f}（更均匀）')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Head Attention\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$\n",
    "\n",
    "将 $d_{model}$ 维的输入拆分成 $h$ 个 $d_k = d_{model}/h$ 维的子空间，\n",
    "每个 head 在自己的子空间中独立计算注意力，最后拼接并线性投影。\n",
    "\n",
    "**参数量**：$W^Q, W^K, W^V, W^O$ 各 $d_{model} \\times d_{model}$，共 $4 d_{model}^2$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 从零实现 Multi-Head Attention ========\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"手写 Multi-Head Attention，不使用 nn.MultiheadAttention\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, 'd_model 必须能被 n_heads 整除'\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # 每个 head 的维度\n",
    "        \n",
    "        # Q, K, V 投影矩阵（合并为一个大矩阵更高效，但这里为清晰分开写）\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)  # (d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)  # 输出投影\n",
    "    \n",
    "    def forward(self, x, mask=None, kv_cache=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: (seq_len, seq_len) 因果掩码\n",
    "            kv_cache: (cached_K, cached_V) 用于推理加速\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attn_weights: (batch, n_heads, seq_len, seq_len)\n",
    "            new_kv_cache: (K, V) 更新后的 cache\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Step 1: 线性投影\n",
    "        Q = self.W_q(x)  # (B, T, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # 如果有 KV Cache（推理时），拼接历史 K, V\n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=1)\n",
    "            V = torch.cat([kv_cache[1], V], dim=1)\n",
    "        new_kv_cache = (K, V)\n",
    "        \n",
    "        # Step 2: 拆分成多个 head\n",
    "        # (B, T, d_model) -> (B, T, n_heads, d_k) -> (B, n_heads, T, d_k)\n",
    "        Q = Q.view(B, Q.size(1), self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(B, K.size(1), self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(B, V.size(1), self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3: 计算 Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask 形状适配 (1, 1, T_q, T_k)\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (B, n_heads, T_q, T_k)\n",
    "        output = torch.matmul(attn_weights, V)     # (B, n_heads, T_q, d_k)\n",
    "        \n",
    "        # Step 4: 拼接所有 head\n",
    "        # (B, n_heads, T, d_k) -> (B, T, n_heads, d_k) -> (B, T, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, -1, self.d_model)\n",
    "        \n",
    "        # Step 5: 输出投影\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attn_weights, new_kv_cache\n",
    "\n",
    "# ======== 验证 ========\n",
    "set_seed(42)\n",
    "d_model, n_heads = 64, 4\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "x = torch.randn(2, 8, d_model)\n",
    "output, weights, _ = mha(x)\n",
    "\n",
    "print(f'输入形状: {x.shape}')\n",
    "print(f'输出形状: {output.shape}  (应为 [2, 8, 64])')\n",
    "print(f'注意力权重形状: {weights.shape}  (应为 [2, 4, 8, 8])')\n",
    "assert output.shape == (2, 8, 64)\n",
    "assert weights.shape == (2, 4, 8, 8)\n",
    "\n",
    "# 参数量验证\n",
    "param_count = sum(p.numel() for p in mha.parameters())\n",
    "expected = 4 * d_model * d_model  # W_q + W_k + W_v + W_o\n",
    "print(f'\\n参数量: {param_count} (预期 4 × {d_model}² = {expected})')\n",
    "assert param_count == expected\n",
    "print('✓ MHA 验证通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 位置编码\n",
    "\n",
    "Self-Attention 本身是**置换不变**的——打乱输入序列的顺序，输出也只是相应打乱。\n",
    "位置编码为模型注入序列顺序信息。\n",
    "\n",
    "我们实现两种方案：\n",
    "1. **Sinusoidal PE**（原始 Transformer）：固定的正弦/余弦函数\n",
    "2. **RoPE**（LLaMA 系列）：旋转位置编码，在 Q·K^T 计算中隐式编码相对位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Sinusoidal 位置编码 ========\n",
    "\n",
    "class SinusoidalPE(nn.Module):\n",
    "    \"\"\"\n",
    "    原始 Transformer 的正弦位置编码\n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len, 1)\n",
    "        \n",
    "        # 计算频率项: 10000^(2i/d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model/2,)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度\n",
    "        \n",
    "        # 注册为 buffer（不参与梯度更新）\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (batch, seq_len, d_model) -> 加上位置编码\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# ======== 可视化 Sinusoidal PE ========\n",
    "pe_module = SinusoidalPE(d_model=64, max_len=128)\n",
    "pe_matrix = pe_module.pe[0].numpy()  # (128, 64)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 热力图\n",
    "im = axes[0].imshow(pe_matrix[:64, :], aspect='auto', cmap='RdBu_r')\n",
    "axes[0].set_xlabel('编码维度')\n",
    "axes[0].set_ylabel('位置')\n",
    "axes[0].set_title('Sinusoidal 位置编码矩阵')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# 选取几个维度展示波形\n",
    "for dim in [0, 1, 10, 11, 30, 31]:\n",
    "    axes[1].plot(pe_matrix[:64, dim], label=f'dim {dim}', alpha=0.7)\n",
    "axes[1].set_xlabel('位置')\n",
    "axes[1].set_ylabel('编码值')\n",
    "axes[1].set_title('不同维度的编码波形（低频→高频）')\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('观察：低维度是低频正弦/余弦（变化慢），高维度是高频（变化快）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== RoPE 旋转位置编码 ========\n",
    "\n",
    "def precompute_rope_freqs(d_model, max_len=512, base=10000.0):\n",
    "    \"\"\"\n",
    "    预计算 RoPE 的频率张量\n",
    "    RoPE 的核心思想：将 Q/K 的每两个相邻维度看成一个 2D 平面上的向量，\n",
    "    按位置旋转不同的角度。位置越远，旋转角度差越大，Q·K 的内积越小。\n",
    "    \"\"\"\n",
    "    # 频率: theta_i = 1 / (base^(2i/d)), i = 0, 1, ..., d/2-1\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "    # 位置: 0, 1, 2, ..., max_len-1\n",
    "    t = torch.arange(max_len).float()\n",
    "    # 外积: (max_len, d/2)\n",
    "    angles = torch.outer(t, freqs)\n",
    "    # 复数形式: e^(i*theta) = cos(theta) + i*sin(theta)\n",
    "    freqs_cis = torch.polar(torch.ones_like(angles), angles)\n",
    "    return freqs_cis  # (max_len, d/2) 复数张量\n",
    "\n",
    "\n",
    "def apply_rope(x, freqs_cis):\n",
    "    \"\"\"\n",
    "    对 Q 或 K 应用旋转位置编码\n",
    "    x: (batch, n_heads, seq_len, d_k)\n",
    "    freqs_cis: (seq_len, d_k/2) 复数张量\n",
    "    \"\"\"\n",
    "    # 将实数张量转为复数: 每两个相邻维度组成一个复数\n",
    "    # (B, H, T, d_k) -> (B, H, T, d_k/2, 2) -> (B, H, T, d_k/2) 复数\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    \n",
    "    # 旋转: 逐元素复数乘法\n",
    "    # freqs_cis 需要广播到 (1, 1, T, d_k/2)\n",
    "    freqs = freqs_cis[:x.size(2)].unsqueeze(0).unsqueeze(0)\n",
    "    x_rotated = x_complex * freqs\n",
    "    \n",
    "    # 转回实数: (B, H, T, d_k/2) 复数 -> (B, H, T, d_k)\n",
    "    return torch.view_as_real(x_rotated).reshape(*x.shape).type_as(x)\n",
    "\n",
    "\n",
    "# ======== 验证 RoPE 的相对位置性质 ========\n",
    "d_k = 16\n",
    "freqs = precompute_rope_freqs(d_k, max_len=64)\n",
    "\n",
    "# 创建两个位置上的 Q 和 K\n",
    "q = torch.randn(1, 1, 1, d_k)  # 某个 query\n",
    "k = torch.randn(1, 1, 1, d_k)  # 某个 key\n",
    "\n",
    "# 在不同绝对位置上计算 Q·K，验证内积只取决于相对位置\n",
    "dots = []\n",
    "for offset in range(20):\n",
    "    # q 在位置 offset, k 在位置 offset+5（相对距离固定为 5）\n",
    "    freqs_q = precompute_rope_freqs(d_k, max_len=64)\n",
    "    freqs_k = precompute_rope_freqs(d_k, max_len=64)\n",
    "    \n",
    "    q_rot = apply_rope(q, freqs_q[offset:offset+1].unsqueeze(0))\n",
    "    k_rot = apply_rope(k, freqs_k[offset+5:offset+6].unsqueeze(0))\n",
    "    \n",
    "    dot = (q_rot * k_rot).sum().item()\n",
    "    dots.append(dot)\n",
    "\n",
    "print('RoPE 相对位置验证（固定相对距离=5，变化绝对位置）:')\n",
    "print(f'Q·K 内积: {[f\"{d:.4f}\" for d in dots[:5]]} ...')\n",
    "print(f'标准差: {np.std(dots):.6f} (接近 0 说明内积只取决于相对位置)')\n",
    "print('✓ RoPE 验证通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Feed-Forward Network\n",
    "\n",
    "Transformer 中每个 block 的另一半是 FFN。两种常见变体：\n",
    "\n",
    "**原始 ReLU FFN**：$\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2$\n",
    "\n",
    "**SwiGLU FFN**（LLaMA 使用）：$\\text{FFN}(x) = (\\text{Swish}(x W_{gate}) \\odot (x W_{up})) W_{down}$\n",
    "\n",
    "SwiGLU 多了一个 gate 矩阵，参数量从 $2 d_{model} d_{ff}$ 变为 $3 d_{model} d_{ff}$，但通常配合更小的 $d_{ff}$ 使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== ReLU FFN ========\n",
    "class ReLUFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(F.relu(self.w1(x)))\n",
    "\n",
    "# ======== SwiGLU FFN ========\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w_gate = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w_up = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w_down = nn.Linear(d_ff, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SwiGLU: Swish(x @ W_gate) * (x @ W_up) @ W_down\n",
    "        return self.w_down(F.silu(self.w_gate(x)) * self.w_up(x))\n",
    "\n",
    "# ======== 参数量对比 ========\n",
    "d_model, d_ff = 64, 256\n",
    "relu_ffn = ReLUFFN(d_model, d_ff)\n",
    "swiglu_ffn = SwiGLUFFN(d_model, d_ff)\n",
    "\n",
    "relu_params = sum(p.numel() for p in relu_ffn.parameters())\n",
    "swiglu_params = sum(p.numel() for p in swiglu_ffn.parameters())\n",
    "print(f'ReLU FFN 参数量:   {relu_params:,}  (2 × {d_model} × {d_ff} + bias)')\n",
    "print(f'SwiGLU FFN 参数量: {swiglu_params:,}  (3 × {d_model} × {d_ff}, no bias)')\n",
    "\n",
    "# 验证输出形状\n",
    "x = torch.randn(2, 8, d_model)\n",
    "assert relu_ffn(x).shape == (2, 8, d_model)\n",
    "assert swiglu_ffn(x).shape == (2, 8, d_model)\n",
    "print('✓ FFN 验证通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Transformer Block + 完整 MiniGPT\n",
    "\n",
    "将所有组件组装成一个完整的 Decoder-only Transformer（GPT 架构）：\n",
    "\n",
    "```\n",
    "输入 tokens\n",
    "    ↓\n",
    "Token Embedding + 位置编码\n",
    "    ↓\n",
    "┌─────────────────┐ × N layers\n",
    "│  RMSNorm         │\n",
    "│  Multi-Head Attn │\n",
    "│  + Residual      │\n",
    "│  RMSNorm         │\n",
    "│  FFN (SwiGLU)    │\n",
    "│  + Residual      │\n",
    "└─────────────────┘\n",
    "    ↓\n",
    "RMSNorm\n",
    "    ↓\n",
    "LM Head (线性投影到词表)\n",
    "    ↓\n",
    "Logits\n",
    "```\n",
    "\n",
    "使用 **Pre-LN**（先归一化再 Attention/FFN）而非 Post-LN，这是 LLaMA 等现代模型的标准做法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== RMSNorm ========\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"LLaMA 使用的 RMSNorm，比 LayerNorm 更轻量（无 mean 减法）\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "\n",
    "# ======== Transformer Block (Pre-LN) ========\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, use_swiglu=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = SwiGLUFFN(d_model, d_ff) if use_swiglu else ReLUFFN(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x, mask=None, kv_cache=None):\n",
    "        # Pre-LN: Norm → Attention → Residual\n",
    "        normed = self.norm1(x)\n",
    "        attn_out, attn_weights, new_cache = self.attn(normed, mask=mask, kv_cache=kv_cache)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # Pre-LN: Norm → FFN → Residual\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x, attn_weights, new_cache\n",
    "\n",
    "\n",
    "# ======== 完整 MiniGPT ========\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_len=512,\n",
    "                 use_pe=True, use_swiglu=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_pe = use_pe\n",
    "        \n",
    "        # Token Embedding\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 位置编码（可选，用于消融实验）\n",
    "        if use_pe:\n",
    "            self.pos_enc = SinusoidalPE(d_model, max_len)\n",
    "        \n",
    "        # N 个 Transformer Block\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, use_swiglu)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # 最终归一化 + LM Head\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # 权重初始化\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, kv_caches=None):\n",
    "        \"\"\"\n",
    "        idx: (batch, seq_len) token indices\n",
    "        Returns: logits (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        x = self.token_emb(idx)  # (B, T, d_model)\n",
    "        \n",
    "        if self.use_pe:\n",
    "            x = self.pos_enc(x)\n",
    "        \n",
    "        # 因果掩码：上三角矩阵（mask=True 的位置不可见）\n",
    "        if kv_caches is None:\n",
    "            mask = torch.triu(torch.ones(T, T, device=idx.device), diagonal=1)\n",
    "        else:\n",
    "            mask = None  # 使用 KV Cache 时只有一个 token，不需要 mask\n",
    "        \n",
    "        # 逐层前向传播\n",
    "        new_caches = []\n",
    "        all_weights = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache = kv_caches[i] if kv_caches is not None else None\n",
    "            x, attn_w, new_cache = layer(x, mask=mask, kv_cache=cache)\n",
    "            new_caches.append(new_cache)\n",
    "            all_weights.append(attn_w)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        return logits, new_caches, all_weights\n",
    "\n",
    "\n",
    "# ======== 实例化并打印模型信息 ========\n",
    "model_config = dict(\n",
    "    vocab_size=128,   # ASCII 字符集\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    d_ff=256,\n",
    "    max_len=256,\n",
    ")\n",
    "\n",
    "model = MiniGPT(**model_config).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'MiniGPT 模型参数量: {total_params:,}')\n",
    "print(f'配置: d_model={model_config[\"d_model\"]}, n_heads={model_config[\"n_heads\"]}, '\n",
    "      f'n_layers={model_config[\"n_layers\"]}, d_ff={model_config[\"d_ff\"]}')\n",
    "\n",
    "# 前向验证\n",
    "dummy_input = torch.randint(0, 128, (2, 32)).to(device)\n",
    "logits, _, _ = model(dummy_input)\n",
    "print(f'\\n输入: {dummy_input.shape} → 输出 Logits: {logits.shape}')\n",
    "assert logits.shape == (2, 32, 128)\n",
    "print('✓ MiniGPT 前向传播验证通过')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Causal Mask + KV Cache\n",
    "\n",
    "**因果掩码**：上三角矩阵，确保位置 $t$ 只能看到 $t$ 及之前的 token。\n",
    "\n",
    "**KV Cache**：自回归生成时，每一步只新增一个 token。之前的 K、V 都已经算过了，\n",
    "缓存起来可以避免重复计算，将推理从 $O(T^2)$ 变为 $O(T)$（每步只算新 token 的 Q·K 和加权 V）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 可视化因果掩码 ========\n",
    "T = 8\n",
    "causal_mask = torch.triu(torch.ones(T, T), diagonal=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# 掩码矩阵\n",
    "axes[0].imshow(causal_mask.numpy(), cmap='Reds')\n",
    "axes[0].set_title('因果掩码（红色=被遮蔽）')\n",
    "axes[0].set_xlabel('Key 位置')\n",
    "axes[0].set_ylabel('Query 位置')\n",
    "for i in range(T):\n",
    "    for j in range(T):\n",
    "        text = '✗' if causal_mask[i, j] else '✓'\n",
    "        axes[0].text(j, i, text, ha='center', va='center', fontsize=10)\n",
    "\n",
    "# 应用掩码后的 attention 权重\n",
    "set_seed(42)\n",
    "Q = torch.randn(1, T, 16)\n",
    "K = torch.randn(1, T, 16)\n",
    "V = torch.randn(1, T, 16)\n",
    "_, causal_attn = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "axes[1].imshow(causal_attn[0].detach().numpy(), cmap='Blues')\n",
    "axes[1].set_title('应用因果掩码后的注意力权重')\n",
    "axes[1].set_xlabel('Key 位置')\n",
    "axes[1].set_ylabel('Query 位置')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('观察：下三角模式——每个位置只关注自己和之前的 token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== KV Cache 推理速度对比 ========\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_without_cache(model, start_tokens, max_new_tokens):\n",
    "    \"\"\"朴素生成：每一步重新计算完整序列\"\"\"\n",
    "    tokens = start_tokens.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _, _ = model(tokens)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    return tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_cache(model, start_tokens, max_new_tokens):\n",
    "    \"\"\"KV Cache 生成：缓存历史 K、V，每步只计算新 token\"\"\"\n",
    "    # 第一步：处理完整的 prompt（prefill）\n",
    "    tokens = start_tokens.clone()\n",
    "    logits, kv_caches, _ = model(tokens)\n",
    "    next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "    tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    # 后续步骤：每次只输入最新一个 token + KV Cache\n",
    "    for _ in range(max_new_tokens - 1):\n",
    "        logits, kv_caches, _ = model(next_token, kv_caches=kv_caches)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# 速度对比\n",
    "model.eval()\n",
    "prompt = torch.randint(0, 128, (1, 16)).to(device)\n",
    "gen_len = 64\n",
    "\n",
    "# 预热\n",
    "_ = generate_without_cache(model, prompt, 5)\n",
    "_ = generate_with_cache(model, prompt, 5)\n",
    "\n",
    "# 无 Cache\n",
    "start = time.time()\n",
    "out_no_cache = generate_without_cache(model, prompt, gen_len)\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "# 有 Cache\n",
    "start = time.time()\n",
    "out_with_cache = generate_with_cache(model, prompt, gen_len)\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f'生成 {gen_len} 个 token:')\n",
    "print(f'  无 KV Cache: {time_no_cache:.3f}s')\n",
    "print(f'  有 KV Cache: {time_with_cache:.3f}s')\n",
    "print(f'  加速比: {time_no_cache / time_with_cache:.2f}x')\n",
    "\n",
    "# 验证两种方式输出一致\n",
    "match = (out_no_cache == out_with_cache).all().item()\n",
    "print(f'  输出一致: {\"✓\" if match else \"✗\"}')\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: 数据准备\n",
    "\n",
    "使用**字符级语言建模**：模型逐字符预测下一个字符。\n",
    "\n",
    "训练数据由简单的重复模式和英文文本组成，模型需要学习这些模式来预测下一个字符。\n",
    "这是一个极简版的 Next-Token Prediction 预训练任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 字符级语言建模数据集 ========\n",
    "\n",
    "# 生成训练文本：混合重复模式 + 简单英文\n",
    "train_text = \"\"\n",
    "# 模式 1: 字母重复 (模型应学会预测重复模式)\n",
    "for _ in range(200):\n",
    "    train_text += \"abcdefg\" * 5 + \" \"\n",
    "# 模式 2: 数字递增\n",
    "for _ in range(200):\n",
    "    train_text += \"0123456789\" * 3 + \" \"\n",
    "# 模式 3: 简单英文句子\n",
    "sentences = [\n",
    "    \"the cat sat on the mat \",\n",
    "    \"the dog ran in the park \",\n",
    "    \"a bird flew over the tree \",\n",
    "    \"the sun is bright today \",\n",
    "    \"hello world this is a test \",\n",
    "]\n",
    "for _ in range(500):\n",
    "    train_text += sentences[np.random.randint(len(sentences))]\n",
    "\n",
    "# 构建字符级词表\n",
    "chars = sorted(set(train_text))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "# 编码\n",
    "data = torch.tensor([char_to_idx[c] for c in train_text], dtype=torch.long)\n",
    "\n",
    "print(f'训练文本长度: {len(train_text):,} 字符')\n",
    "print(f'词表大小: {vocab_size} (字符集: {\"\".join(chars[:20])}...)')\n",
    "print(f'编码示例: \"{train_text[:30]}\" → {data[:30].tolist()}')\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"字符级语言建模数据集：输入 x[t]，目标 x[t+1]\"\"\"\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + 1 : idx + self.seq_len + 1]\n",
    "        return x, y\n",
    "\n",
    "seq_len = 64\n",
    "dataset = CharDataset(data, seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(f'\\n数据集大小: {len(dataset)} 条序列')\n",
    "print(f'序列长度: {seq_len}')\n",
    "print(f'Batch 数: {len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: 训练 MiniGPT\n",
    "\n",
    "标准的 Causal Language Modeling 训练循环：\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{<t})$$\n",
    "\n",
    "使用 AdamW 优化器 + 余弦退火学习率调度（与真实预训练一致）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 训练函数 ========\n",
    "\n",
    "def train_model(model, dataloader, n_epochs=10, lr=3e-4, device='cpu'):\n",
    "    \"\"\"训练 MiniGPT 并返回 loss 历史\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1,\n",
    "                                  betas=(0.9, 0.95))  # LLaMA 风格超参\n",
    "    \n",
    "    # 余弦退火调度器\n",
    "    total_steps = n_epochs * len(dataloader)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=lr * 0.1)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            logits, _, _ = model(x)  # (B, T, vocab_size)\n",
    "            \n",
    "            # Cross-Entropy Loss（展平 batch 和 seq_len 维度）\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),  # (B*T, vocab_size)\n",
    "                y.view(-1)                          # (B*T,)\n",
    "            )\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪（防止梯度爆炸）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss_history.append(loss.item())\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}, '\n",
    "                  f'LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 训练主模型 ========\n",
    "set_seed(42)\n",
    "\n",
    "# 用实际词表大小重建模型\n",
    "model_config['vocab_size'] = vocab_size\n",
    "model = MiniGPT(**model_config).to(device)\n",
    "print(f'模型参数量: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'词表大小: {vocab_size}\\n')\n",
    "\n",
    "loss_history = train_model(model, dataloader, n_epochs=15, lr=3e-4, device=device)\n",
    "\n",
    "# 绘制 Loss 曲线\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(loss_history, alpha=0.3, color='blue', label='每步 Loss')\n",
    "# 滑动平均\n",
    "window = 20\n",
    "smoothed = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(loss_history)), smoothed, color='red', linewidth=2, label=f'滑动平均 (窗口={window})')\n",
    "plt.xlabel('训练步数')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('MiniGPT 训练 Loss 曲线')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n初始 Loss: {loss_history[0]:.4f}')\n",
    "print(f'最终 Loss: {np.mean(loss_history[-20:]):.4f}')\n",
    "print(f'理论随机基线 (1/vocab_size): {-math.log(1/vocab_size):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: 消融实验\n",
    "\n",
    "通过对比实验验证各组件的贡献：\n",
    "\n",
    "- **实验 A**：有位置编码 vs 无位置编码 → 验证位置信息的重要性\n",
    "- **实验 B**：SwiGLU vs ReLU FFN → 对比激活函数的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 消融实验 ========\n",
    "\n",
    "ablation_results = {}\n",
    "n_epochs_ablation = 10  # 消融实验用较少 epoch 以节省时间\n",
    "\n",
    "# 实验 A: 无位置编码\n",
    "print('=' * 50)\n",
    "print('消融实验 A: 无位置编码')\n",
    "print('=' * 50)\n",
    "set_seed(42)\n",
    "config_no_pe = {**model_config, 'use_pe': False}\n",
    "model_no_pe = MiniGPT(**config_no_pe).to(device)\n",
    "ablation_results['无位置编码'] = train_model(\n",
    "    model_no_pe, dataloader, n_epochs=n_epochs_ablation, lr=3e-4, device=device\n",
    ")\n",
    "\n",
    "# 实验 B: ReLU FFN（替代 SwiGLU）\n",
    "print('\\n' + '=' * 50)\n",
    "print('消融实验 B: ReLU FFN (替代 SwiGLU)')\n",
    "print('=' * 50)\n",
    "set_seed(42)\n",
    "config_relu = {**model_config, 'use_swiglu': False}\n",
    "model_relu = MiniGPT(**config_relu).to(device)\n",
    "ablation_results['ReLU FFN'] = train_model(\n",
    "    model_relu, dataloader, n_epochs=n_epochs_ablation, lr=3e-4, device=device\n",
    ")\n",
    "\n",
    "# 基线（完整模型）\n",
    "ablation_results['完整模型 (PE + SwiGLU)'] = loss_history[:len(ablation_results['无位置编码'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 消融结果可视化 ========\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "window = 20\n",
    "colors = {'完整模型 (PE + SwiGLU)': 'green', '无位置编码': 'red', 'ReLU FFN': 'orange'}\n",
    "\n",
    "for name, losses in ablation_results.items():\n",
    "    smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(smoothed, label=f'{name} (最终: {np.mean(losses[-20:]):.3f})',\n",
    "             color=colors.get(name, 'blue'), linewidth=2)\n",
    "\n",
    "plt.xlabel('训练步数')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('消融实验：各组件对训练的影响')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print('消融实验结论:')\n",
    "for name, losses in ablation_results.items():\n",
    "    print(f'  {name}: 最终 Loss = {np.mean(losses[-20:]):.4f}')\n",
    "print('\\n预期观察：')\n",
    "print('- 无位置编码的模型 Loss 最高（无法利用序列顺序信息）')\n",
    "print('- SwiGLU 通常略优于 ReLU（门控机制更灵活）')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: 文本生成演示\n",
    "\n",
    "用训练好的 MiniGPT 进行自回归生成。展示 temperature 参数对生成多样性的影响：\n",
    "- **temperature = 0**（greedy）：总是选概率最高的 token，输出确定性\n",
    "- **temperature = 0.5**：适度多样性\n",
    "- **temperature = 1.0**：标准采样\n",
    "- **temperature = 2.0**：高多样性但可能出现乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 文本生成 ========\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, prompt_str, max_new_tokens=100, temperature=1.0):\n",
    "    \"\"\"使用 KV Cache 进行自回归生成\"\"\"\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([[char_to_idx.get(c, 0) for c in prompt_str]], device=device)\n",
    "    \n",
    "    # Prefill: 处理完整 prompt\n",
    "    logits, kv_caches, _ = model(tokens)\n",
    "    \n",
    "    generated = list(prompt_str)\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 取最后一个位置的 logits\n",
    "        next_logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "        \n",
    "        if temperature == 0:  # Greedy\n",
    "            next_token = next_logits.argmax(dim=-1, keepdim=True)\n",
    "        else:  # 采样\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        generated.append(idx_to_char.get(next_token.item(), '?'))\n",
    "        \n",
    "        # 增量推理: 只输入新 token + KV Cache\n",
    "        logits, kv_caches, _ = model(next_token, kv_caches=kv_caches)\n",
    "    \n",
    "    model.train()\n",
    "    return ''.join(generated)\n",
    "\n",
    "\n",
    "# 不同 temperature 的生成对比\n",
    "prompt = \"abcde\"\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "for temp in [0, 0.5, 1.0, 2.0]:\n",
    "    result = generate_text(model, prompt, max_new_tokens=60, temperature=temp)\n",
    "    print(f'Temperature={temp}: \"{result}\"')\n",
    "\n",
    "print('\\n---')\n",
    "prompt2 = \"the cat\"\n",
    "print(f'\\nPrompt: \"{prompt2}\"\\n')\n",
    "for temp in [0, 0.5, 1.0]:\n",
    "    result = generate_text(model, prompt2, max_new_tokens=60, temperature=temp)\n",
    "    print(f'Temperature={temp}: \"{result}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: 注意力权重可视化\n",
    "\n",
    "可视化不同层、不同 head 的注意力权重，观察模型学到了什么样的注意力模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 注意力权重可视化 ========\n",
    "\n",
    "model.eval()\n",
    "vis_text = \"abcdefgabcdefg\"\n",
    "vis_tokens = torch.tensor([[char_to_idx.get(c, 0) for c in vis_text]], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, _, all_attn_weights = model(vis_tokens)\n",
    "\n",
    "# 绘制前 2 层 × 4 个 head 的注意力权重\n",
    "n_layers_to_show = min(2, len(all_attn_weights))\n",
    "n_heads_to_show = min(4, all_attn_weights[0].size(1))\n",
    "\n",
    "fig, axes = plt.subplots(n_layers_to_show, n_heads_to_show, \n",
    "                          figsize=(4 * n_heads_to_show, 4 * n_layers_to_show))\n",
    "if n_layers_to_show == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "labels = list(vis_text)\n",
    "\n",
    "for layer_idx in range(n_layers_to_show):\n",
    "    for head_idx in range(n_heads_to_show):\n",
    "        attn = all_attn_weights[layer_idx][0, head_idx].cpu().numpy()\n",
    "        ax = axes[layer_idx, head_idx]\n",
    "        im = ax.imshow(attn, cmap='Blues', vmin=0)\n",
    "        ax.set_title(f'Layer {layer_idx}, Head {head_idx}', fontsize=10)\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, fontsize=7)\n",
    "        ax.set_yticklabels(labels, fontsize=7)\n",
    "\n",
    "plt.suptitle('各层各 Head 的注意力权重热力图', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.train()\n",
    "print('观察要点：')\n",
    "print('- 因果掩码效果：下三角模式（未来位置为 0）')\n",
    "print('- 不同 Head 可能关注不同的模式（如相邻 token、重复 token）')\n",
    "print('- 对角线附近通常权重较高（关注附近位置）')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: 实验结论\n",
    "\n",
    "### 核心收获\n",
    "\n",
    "1. **Scaled Dot-Product Attention** 中 $\\sqrt{d_k}$ 缩放至关重要——没有它，高维空间中 softmax 会饱和，梯度消失\n",
    "2. **Multi-Head Attention** 允许模型在不同子空间中捕获不同的注意力模式\n",
    "3. **位置编码** 是 Transformer 感知序列顺序的唯一来源——消融实验证明了去掉它性能显著下降\n",
    "4. **KV Cache** 通过缓存历史 K、V 避免重复计算，是自回归推理的标准优化\n",
    "5. **Causal Mask** 确保模型在训练时不会\"偷看\"未来的 token\n",
    "\n",
    "### 简化版 vs 真实 GPT 的差异\n",
    "\n",
    "| 维度 | 我们的 MiniGPT | 真实 GPT/LLaMA |\n",
    "|------|:-------------:|:--------------:|\n",
    "| 参数量 | ~100K | 7B-405B |\n",
    "| 词表 | 字符级 (~40) | BPE (~32K-128K) |\n",
    "| 位置编码 | Sinusoidal | RoPE |\n",
    "| 训练数据 | 几 KB 重复文本 | 数万亿 token 互联网数据 |\n",
    "| 训练时间 | 几分钟 (CPU) | 数月 (数千 GPU) |\n",
    "| 归一化 | RMSNorm (Pre-LN) | RMSNorm (Pre-LN) ✓ |\n",
    "| FFN | SwiGLU | SwiGLU ✓ |\n",
    "| 注意力 | MHA | GQA (Grouped-Query Attention) |\n",
    "| 优化器 | AdamW | AdamW ✓ |\n",
    "\n",
    "### 与理论笔记的对照\n",
    "\n",
    "本实验代码实现了 [Transformer 架构详解](../../notes/fundamentals/transformer.md) 中讨论的几乎所有核心组件：\n",
    "- Self-Attention 的 $QK^T/\\sqrt{d_k}$ 缩放 → 用可视化证明了笔记中的方差分析\n",
    "- Multi-Head Attention 的 split + concat → 参数量公式 $4d^2$ 得到验证\n",
    "- Pre-LN vs Post-LN → 消融实验可以观察到区别\n",
    "- SwiGLU FFN → 参数量 $3d \\cdot d_{ff}$ 得到验证\n",
    "- KV Cache → 速度对比直观展示了理论分析中的加速效果\n",
    "\n",
    "### 后续实验建议\n",
    "\n",
    "- 实现 GQA (Grouped-Query Attention) 并对比 MHA\n",
    "- 实现 Flash Attention 的 tiling + online softmax\n",
    "- 使用更大的数据集（如 tiny Shakespeare）训练更大的模型\n",
    "- 实现 RoPE 位置编码的完整版本并替换 Sinusoidal PE\n",
    "- 实现 Gradient Checkpointing 对比显存占用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
