{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 从零实现简化版多模态预训练模型：理解 FSDP 分布式训练\n\n> 实验目标：构建一个简化版 CLIP 模型，在 CIFAR-10 上完成对比预训练，并深入理解 FSDP（Fully Sharded Data Parallel）分布式训练的工作原理。\n\n## 实验目标\n\n1. **从零构建简化版 CLIP 模型**：Vision Encoder (ViT) + Text Encoder (Transformer) + Contrastive Loss\n2. **在 CIFAR-10 + 模板 Caption 上完成单 GPU 对比学习训练**，验证对比学习能否实现零样本分类\n3. **深入理解 FSDP 的核心概念**：参数分片（Sharding）、梯度同步、混合精度、Checkpoint 保存\n4. **掌握从单 GPU 代码迁移到 FSDP 分布式训练的完整流程**\n\n## 预期结果\n\n- 训练 20 个 epoch 后，Contrastive Loss 从 ~4.0 稳步降至 ~1.5\n- 零样本分类准确率在 CIFAR-10 测试集上达到 ~50-70%（远超随机猜测的 10%）\n- t-SNE 可视化中，同类图像嵌入和对应文本嵌入聚集在一起\n\n## 所需环境\n\n```\nPython >= 3.9\nPyTorch >= 2.0（FSDP 改进版 API）\ntorchvision\nmatplotlib\nnumpy\nscikit-learn（用于 t-SNE 可视化）\n```"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport os\n\n# 设置随机种子，确保实验可复现\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed()\n\n# 检测设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'使用设备: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name()}')\n    print(f'显存: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: 数据准备\n\n为了快速验证多模态对比学习的效果，我们使用 **CIFAR-10 数据集**配合**模板生成的 Caption**。\n\n这种做法模拟了 CLIP 的训练方式：每张图像配有一段描述性文本。虽然我们的 Caption 是模板生成的（如 `\"a photo of a cat\"`），但足以验证对比学习能否让模型学会图文对齐。\n\n**与 CLIP 原版的区别**：\n- CLIP 使用 4 亿个从互联网收集的真实图文对\n- 我们使用 5 万张 CIFAR-10 图像 + 8 种 Caption 模板\n- 训练目标相同：让匹配的图文对在嵌入空间中靠近"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ======== CIFAR-10 类别定义 ========\nCIFAR10_CLASSES = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer',\n    'dog', 'frog', 'horse', 'ship', 'truck'\n]\n\n# ======== Caption 模板 ========\n# 模拟 CLIP 论文中的 Prompt Engineering:\n# 使用多种模板增加文本多样性，避免模型过拟合到单一模板\nCAPTION_TEMPLATES = [\n    'a photo of a {}',\n    'a picture of a {}',\n    'an image showing a {}',\n    'a {} in a photograph',\n    'a blurry photo of a {}',\n    'a close-up of a {}',\n    'a bright photo of a {}',\n    'a dark photo of a {}',\n]\n\n\nclass SimpleTokenizer:\n    \"\"\"简易词级别分词器\n\n    在真实 CLIP 中使用 BPE (Byte Pair Encoding) 分词，\n    这里为了教学目的使用简单的词级别分词。\n    词表从所有可能的 caption 中自动构建。\n    \"\"\"\n\n    def __init__(self, max_len=12):\n        self.max_len = max_len\n        # 从所有可能的 caption 中构建词表\n        words = set()\n        for cls_name in CIFAR10_CLASSES:\n            for template in CAPTION_TEMPLATES:\n                for word in template.format(cls_name).split():\n                    words.add(word)\n        # 特殊 token: <pad>=0 用于填充, <bos>=1 句首, <eos>=2 句尾\n        self.word2idx = {'<pad>': 0, '<bos>': 1, '<eos>': 2}\n        for i, word in enumerate(sorted(words), start=3):\n            self.word2idx[word] = i\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.vocab_size = len(self.word2idx)\n\n    def encode(self, text):\n        \"\"\"将文本编码为 token id 序列\"\"\"\n        tokens = [self.word2idx['<bos>']]\n        tokens += [self.word2idx.get(w, 0) for w in text.split()]\n        tokens += [self.word2idx['<eos>']]\n        # 填充或截断到固定长度\n        if len(tokens) < self.max_len:\n            tokens += [0] * (self.max_len - len(tokens))\n        else:\n            tokens = tokens[:self.max_len]\n        return tokens\n\n\nclass CIFAR10WithCaptions(Dataset):\n    \"\"\"带文本描述的 CIFAR-10 数据集\n\n    每次取样时随机选择一个 Caption 模板，\n    模拟真实训练中图文对的多样性。\n    \"\"\"\n\n    def __init__(self, root='./data', train=True, tokenizer=None):\n        self.dataset = datasets.CIFAR10(\n            root=root, train=train, download=True,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    (0.4914, 0.4822, 0.4465),\n                    (0.2470, 0.2435, 0.2616)\n                )\n            ])\n        )\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        # 随机选择一个 caption 模板，增加数据多样性\n        template = random.choice(CAPTION_TEMPLATES)\n        caption = template.format(CIFAR10_CLASSES[label])\n        tokens = torch.tensor(self.tokenizer.encode(caption), dtype=torch.long)\n        return image, tokens, label\n\n\n# ======== 创建分词器和数据集 ========\ntokenizer = SimpleTokenizer(max_len=12)\nprint(f'词表大小: {tokenizer.vocab_size}')\nprint(f'编码示例: \"a photo of a cat\" -> {tokenizer.encode(\"a photo of a cat\")}')\n\ntrain_dataset = CIFAR10WithCaptions(root='./data', train=True, tokenizer=tokenizer)\ntest_dataset = CIFAR10WithCaptions(root='./data', train=False, tokenizer=tokenizer)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=256, shuffle=True,\n    num_workers=2, pin_memory=True, drop_last=True  # drop_last 保证 batch 对齐\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=256, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\nprint(f'训练集: {len(train_dataset)} 样本, {len(train_loader)} 个 batch')\nprint(f'测试集: {len(test_dataset)} 样本')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ======== 可视化几个样本 ========\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nmean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\nstd = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n\nfor i in range(10):\n    img, tokens, label = train_dataset[i * 500]\n    # 反归一化\n    img_display = (img * std + mean).clamp(0, 1).permute(1, 2, 0).numpy()\n    # 解码 caption\n    caption_words = [\n        tokenizer.idx2word.get(t.item(), '')\n        for t in tokens if t.item() not in (0, 1, 2)\n    ]\n    caption = ' '.join(caption_words)\n\n    ax = axes[i // 5][i % 5]\n    ax.imshow(img_display)\n    ax.set_title(caption, fontsize=9)\n    ax.axis('off')\n\nplt.suptitle('CIFAR-10 样本及其模板 Caption', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: 模型架构 — 简化版 CLIP\n\n我们构建一个简化版 CLIP 模型，核心架构如下：\n\n```\n图像 (32×32×3)                                文本 token 序列\n     │                                              │\n     ▼                                              ▼\nPatch Embedding                             Token Embedding\n(4×4 patch → 64 token, 256 维)              + Position Embedding\n     │                                              │\n     ▼                                              ▼\nVision Encoder                               Text Encoder\n(6 层 Transformer)                           (4 层 Transformer)\n     │                                              │\n     ▼                                              ▼\n  CLS Token                                 Mean Pooling\n  (256 维)                                   (256 维)\n     │                                              │\n     ▼                                              ▼\n线性投影 → L2 归一化                    线性投影 → L2 归一化\n     │                                              │\n     ▼                                              ▼\n图像嵌入 (128 维) ──── 余弦相似度 × exp(τ) ──── 文本嵌入 (128 维)\n                            │\n                            ▼\n                     InfoNCE Loss\n```\n\n**设计选择说明**：\n- **patch_size=4**: CIFAR-10 图像仅 32×32，4×4 patch 得到 8×8=64 个 token（CLIP 原版用 14×14 patch 处理 224×224 图像）\n- **embed_dim=256**: 远小于 CLIP 原版的 768/1024，适合在单 GPU 上快速训练\n- **proj_dim=128**: 投影到低维共享空间，减少计算量\n- **6 层视觉 / 4 层文本**: 视觉端更深，因为图像信息量远大于模板 caption"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class PatchEmbedding(nn.Module):\n    \"\"\"将图像分割为不重叠的 patch，并映射到嵌入空间\n\n    对于 32×32 的 CIFAR-10 图像，patch_size=4 得到 8×8=64 个 patch。\n    每个 patch 通过卷积层映射为 embed_dim 维向量。\n    加入可学习的 CLS token 和位置编码。\n    \"\"\"\n\n    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n        super().__init__()\n        self.num_patches = (img_size // patch_size) ** 2  # 64\n        # 用卷积实现 patch 切分 + 线性映射\n        # 等价于: 把图像切成 patch → 展平每个 patch → 全连接层\n        self.proj = nn.Conv2d(\n            in_channels, embed_dim,\n            kernel_size=patch_size, stride=patch_size\n        )\n        # CLS token: 用于聚合全局信息（ViT 标准做法）\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n        # 可学习的绝对位置编码（CLIP 原版也是这种方式）\n        self.pos_embed = nn.Parameter(\n            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02\n        )\n\n    def forward(self, x):\n        B = x.shape[0]\n        # [B, 3, 32, 32] → [B, 256, 8, 8]\n        x = self.proj(x)\n        # 展平空间维度: [B, 256, 8, 8] → [B, 64, 256]\n        x = x.flatten(2).transpose(1, 2)\n        # 拼接 CLS token: [B, 64, 256] → [B, 65, 256]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        # 添加位置编码\n        x = x + self.pos_embed\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"标准 Transformer 编码器层 (Pre-Norm 变体)\n\n    Pre-Norm: 先 LayerNorm 再做 Attention/MLP\n    这是现代 Transformer (GPT-2, ViT, LLaMA) 的标准做法，训练更稳定。\n\n    注意：这个类同时也是 FSDP auto_wrap_policy 的包装单元。\n    FSDP 会以 TransformerBlock 为粒度进行参数分片。\n    \"\"\"\n\n    def __init__(self, embed_dim=256, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim, num_heads, dropout=dropout, batch_first=True\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x, key_padding_mask=None):\n        # Self-Attention + 残差连接\n        normed = self.norm1(x)\n        x = x + self.attn(\n            normed, normed, normed,\n            key_padding_mask=key_padding_mask\n        )[0]\n        # FFN + 残差连接\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass VisionEncoder(nn.Module):\n    \"\"\"简化版 ViT (Vision Transformer)\n\n    与 CLIP 原版的主要区别：\n    - 更小: 6 层 vs 24 层, 256 维 vs 1024 维\n    - 更小的 patch: 4×4 vs 14×14（因为输入图像更小）\n    - 没有 attention pooling（直接取 CLS token）\n    \"\"\"\n\n    def __init__(self, img_size=32, patch_size=4, embed_dim=256,\n                 num_layers=6, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = self.patch_embed(x)  # [B, 65, 256]\n        for block in self.blocks:\n            x = block(x)\n        x = self.norm(x)\n        return x[:, 0]  # 取 CLS token: [B, 256]\n\n\nclass TextEncoder(nn.Module):\n    \"\"\"简化版 Text Transformer\n\n    与 CLIP 原版的区别：\n    - 使用双向注意力（CLIP 原版用因果掩码，类似 GPT）\n    - 使用 mean pooling 而非 EOS token 作为句子表征\n    - 更小: 4 层 vs 12 层\n    \"\"\"\n\n    def __init__(self, vocab_size, max_len=12, embed_dim=256,\n                 num_layers=4, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.pos_embed = nn.Parameter(\n            torch.randn(1, max_len, embed_dim) * 0.02\n        )\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, dropout=dropout)\n            for _ in range(num_layers)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # x: [B, seq_len] token indices\n        padding_mask = (x == 0)  # True for <pad> positions\n        x = self.token_embed(x) + self.pos_embed[:, :x.shape[1]]\n        for block in self.blocks:\n            x = block(x, key_padding_mask=padding_mask)\n        x = self.norm(x)\n        # Mean pooling: 对非 padding token 的隐藏状态取平均\n        mask = (~padding_mask).unsqueeze(-1).float()  # [B, seq_len, 1]\n        x = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n        return x  # [B, 256]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class MiniCLIP(nn.Module):\n    \"\"\"简化版 CLIP 模型\n\n    核心组件：\n    - vision_encoder: 图像编码器 (ViT)\n    - text_encoder: 文本编码器 (Transformer)\n    - vision_proj / text_proj: 将编码器输出投影到共享嵌入空间\n    - logit_scale: 可学习的温度参数 τ（对应 CLIP 论文中的 exp(t)）\n    \"\"\"\n\n    def __init__(self, vocab_size, img_size=32, patch_size=4,\n                 embed_dim=256, proj_dim=128,\n                 v_layers=6, t_layers=4, num_heads=8):\n        super().__init__()\n        self.vision_encoder = VisionEncoder(\n            img_size, patch_size, embed_dim, v_layers, num_heads\n        )\n        self.text_encoder = TextEncoder(\n            vocab_size, max_len=12, embed_dim=embed_dim,\n            num_layers=t_layers, num_heads=num_heads\n        )\n        # 线性投影到共享空间（CLIP 原版也是线性投影，非 MLP）\n        self.vision_proj = nn.Linear(embed_dim, proj_dim, bias=False)\n        self.text_proj = nn.Linear(embed_dim, proj_dim, bias=False)\n        # 可学习温度参数，初始化为 ln(1/0.07) ≈ 2.66\n        # 训练时裁剪上界使 exp(logit_scale) ≤ 100\n        self.logit_scale = nn.Parameter(torch.tensor(np.log(1 / 0.07)))\n\n    def encode_image(self, images):\n        \"\"\"编码图像 → L2 归一化的嵌入向量\"\"\"\n        features = self.vision_encoder(images)\n        return F.normalize(self.vision_proj(features), dim=-1)\n\n    def encode_text(self, tokens):\n        \"\"\"编码文本 → L2 归一化的嵌入向量\"\"\"\n        features = self.text_encoder(tokens)\n        return F.normalize(self.text_proj(features), dim=-1)\n\n    def forward(self, images, tokens):\n        img_embed = self.encode_image(images)   # [B, proj_dim]\n        txt_embed = self.encode_text(tokens)     # [B, proj_dim]\n\n        # 计算缩放后的余弦相似度矩阵\n        logit_scale = self.logit_scale.exp().clamp(max=100.0)\n        logits = logit_scale * img_embed @ txt_embed.t()  # [B, B]\n\n        return logits, img_embed, txt_embed\n\n\ndef contrastive_loss(logits):\n    \"\"\"对称 InfoNCE 损失\n\n    logits: [N, N] 缩放后的余弦相似度矩阵\n    正确配对在对角线上：logits[i, i] 是第 i 张图像和第 i 段文本的匹配分数\n    \"\"\"\n    N = logits.shape[0]\n    labels = torch.arange(N, device=logits.device)\n    # 图像→文本: 每张图像在 N 段文本中找到正确配对\n    loss_i2t = F.cross_entropy(logits, labels)\n    # 文本→图像: 每段文本在 N 张图像中找到正确配对\n    loss_t2i = F.cross_entropy(logits.t(), labels)\n    return (loss_i2t + loss_t2i) / 2\n\n\n# ======== 实例化模型 ========\nmodel = MiniCLIP(vocab_size=tokenizer.vocab_size).to(device)\n\n# 打印参数量\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'总参数量: {total_params:,} ({total_params/1e6:.1f}M)')\nprint()\nfor name, child in model.named_children():\n    n = sum(p.numel() for p in child.parameters())\n    print(f'  {name}: {n:,} ({n/1e6:.2f}M)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: 单 GPU 训练\n\n使用标准的 PyTorch 训练循环，训练 20 个 epoch。\n\n**关键超参数**：\n- **Batch Size = 256**: 对比学习受益于大 batch（提供更多负样本），但受限于 GPU 显存\n- **学习率 = 3e-4**: 配合 AdamW 优化器和余弦退火调度\n- **权重衰减 = 0.05**: 正则化，防止过拟合\n- **梯度裁剪 = 1.0**: 防止训练不稳定\n\n> 注意：这里的训练代码是标准的单 GPU 版本。在 Part 5 中，我们将展示如何将其改造为 FSDP 分布式训练。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def train_one_epoch(model, dataloader, optimizer, epoch, device):\n    \"\"\"训练一个 epoch，返回平均 loss\"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = 0\n\n    for batch_idx, (images, tokens, _) in enumerate(dataloader):\n        images = images.to(device)\n        tokens = tokens.to(device)\n\n        # 前向传播: 计算相似度矩阵和嵌入\n        logits, _, _ = model(images, tokens)\n        loss = contrastive_loss(logits)\n\n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        # 梯度裁剪: 防止梯度爆炸导致训练不稳定\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n        num_batches += 1\n\n        # 每 50 个 batch 打印一次\n        if batch_idx % 50 == 0:\n            scale = model.logit_scale.exp().item()\n            print(f'  [{batch_idx:>3d}/{len(dataloader)}] '\n                  f'Loss: {loss.item():.4f}  '\n                  f'Temperature: {1/scale:.4f}  '\n                  f'Scale: {scale:.1f}')\n\n    return total_loss / num_batches\n\n\n# ======== 训练配置 ========\nEPOCHS = 20\nLR = 3e-4\nWEIGHT_DECAY = 0.05\n\noptimizer = torch.optim.AdamW(\n    model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=EPOCHS, eta_min=1e-6\n)\n\n# ======== 训练循环 ========\ntrain_losses = []\nprint(f'开始训练: {EPOCHS} epochs, batch_size=256, lr={LR}\\n')\n\nfor epoch in range(1, EPOCHS + 1):\n    loss = train_one_epoch(model, train_loader, optimizer, epoch, device)\n    scheduler.step()\n    train_losses.append(loss)\n    current_lr = scheduler.get_last_lr()[0]\n    print(f'Epoch {epoch:>2d}/{EPOCHS} | '\n          f'平均 Loss: {loss:.4f} | '\n          f'LR: {current_lr:.2e}\\n')\n\nprint('训练完成！')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ======== 绘制训练 Loss 曲线 ========\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(train_losses) + 1), train_losses,\n         'b-o', linewidth=2, markersize=5)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Contrastive Loss', fontsize=12)\nplt.title('训练损失曲线', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.xticks(range(1, len(train_losses) + 1))\nplt.tight_layout()\nplt.show()\n\nprint(f'初始 Loss: {train_losses[0]:.4f}')\nprint(f'最终 Loss: {train_losses[-1]:.4f}')\nprint(f'Loss 下降: {train_losses[0] - train_losses[-1]:.4f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: 零样本分类 & 嵌入可视化\n\n### 零样本分类\n\n核心思路（与 CLIP 论文一致）：\n1. 为每个类别生成文本描述 → `\"a photo of a dog\"`\n2. 用 Text Encoder 编码 → 得到 10 个类别的文本嵌入\n3. 对测试图像用 Image Encoder 编码 → 得到图像嵌入\n4. 计算图像嵌入与所有类别文本的余弦相似度 → 选最高的作为预测\n\n如果模型学到了有效的图文对齐，零样本准确率应远超 10%（随机猜测基线）。"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "@torch.no_grad()\ndef zero_shot_eval(model, test_loader, tokenizer, device):\n    \"\"\"零样本分类评估\"\"\"\n    model.eval()\n\n    # 1. 编码所有类别的文本描述\n    class_prompts = [f'a photo of a {cls}' for cls in CIFAR10_CLASSES]\n    class_tokens = torch.stack([\n        torch.tensor(tokenizer.encode(p), dtype=torch.long)\n        for p in class_prompts\n    ]).to(device)\n    class_embeddings = model.encode_text(class_tokens)  # [10, 128]\n\n    # 2. 对测试集逐 batch 预测\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n\n    for images, _, labels in test_loader:\n        images = images.to(device)\n        image_embeddings = model.encode_image(images)  # [B, 128]\n\n        # 计算与每个类别的余弦相似度\n        similarity = image_embeddings @ class_embeddings.t()  # [B, 10]\n        predictions = similarity.argmax(dim=-1).cpu()\n\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n        all_preds.extend(predictions.tolist())\n        all_labels.extend(labels.tolist())\n\n    accuracy = correct / total\n    return accuracy, all_preds, all_labels\n\n\naccuracy, preds, labels = zero_shot_eval(model, test_loader, tokenizer, device)\nprint(f'零样本分类准确率: {accuracy:.1%}  (随机基线: 10.0%)')\n\n# 分类别准确率\nprint(f'\\n各类别准确率:')\nfor i, cls in enumerate(CIFAR10_CLASSES):\n    cls_mask = [j for j, l in enumerate(labels) if l == i]\n    cls_correct = sum(1 for j in cls_mask if preds[j] == i)\n    cls_acc = cls_correct / len(cls_mask) if cls_mask else 0\n    print(f'  {cls:>12s}: {cls_acc:.1%}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ======== t-SNE 嵌入空间可视化 ========\nfrom sklearn.manifold import TSNE\n\n@torch.no_grad()\ndef extract_embeddings(model, test_loader, tokenizer, device, max_samples=1000):\n    \"\"\"提取图像和文本嵌入用于可视化\"\"\"\n    model.eval()\n    img_embeds, labels_list = [], []\n    count = 0\n\n    for images, _, batch_labels in test_loader:\n        if count >= max_samples:\n            break\n        images = images.to(device)\n        img_embed = model.encode_image(images).cpu().numpy()\n        img_embeds.append(img_embed)\n        labels_list.append(batch_labels.numpy())\n        count += images.size(0)\n\n    # 类别文本嵌入\n    class_prompts = [f'a photo of a {cls}' for cls in CIFAR10_CLASSES]\n    class_tokens = torch.stack([\n        torch.tensor(tokenizer.encode(p), dtype=torch.long)\n        for p in class_prompts\n    ]).to(device)\n    txt_embed = model.encode_text(class_tokens).cpu().numpy()\n\n    img_embeds = np.concatenate(img_embeds)[:max_samples]\n    labels_arr = np.concatenate(labels_list)[:max_samples]\n    return img_embeds, txt_embed, labels_arr\n\n\n# 提取嵌入\nimg_embeds, txt_embeds, viz_labels = extract_embeddings(\n    model, test_loader, tokenizer, device, max_samples=1000\n)\n\n# t-SNE 降维: 将图像嵌入和文本嵌入合并后一起降维\nall_embeds = np.vstack([img_embeds, txt_embeds])\ntsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\nall_2d = tsne.fit_transform(all_embeds)\n\nimg_2d = all_2d[:len(img_embeds)]\ntxt_2d = all_2d[len(img_embeds):]\n\n# 绘图\nfig, ax = plt.subplots(1, 1, figsize=(12, 10))\ncolors = plt.cm.tab10(np.linspace(0, 1, 10))\n\n# 图像嵌入: 小圆点\nfor i in range(10):\n    mask = viz_labels == i\n    ax.scatter(img_2d[mask, 0], img_2d[mask, 1],\n               c=[colors[i]], s=10, alpha=0.4, label=CIFAR10_CLASSES[i])\n\n# 文本嵌入: 大星号\nfor i in range(10):\n    ax.scatter(txt_2d[i, 0], txt_2d[i, 1],\n               c=[colors[i]], s=300, marker='*',\n               edgecolors='black', linewidths=1, zorder=10)\n    ax.annotate(CIFAR10_CLASSES[i], (txt_2d[i, 0], txt_2d[i, 1]),\n                fontsize=9, fontweight='bold',\n                xytext=(5, 5), textcoords='offset points')\n\nax.legend(loc='upper right', fontsize=8, markerscale=3)\nax.set_title('t-SNE 可视化: 图像嵌入 (圆点) + 类别文本嵌入 (星号)', fontsize=13)\nax.set_xlabel('t-SNE dim 1')\nax.set_ylabel('t-SNE dim 2')\nplt.tight_layout()\nplt.show()\n\nprint('如果训练有效，你应该看到:')\nprint('  - 同类图像聚成簇')\nprint('  - 每个类别的星号(文本嵌入)位于对应图像簇的中心附近')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 5: FSDP 分布式训练详解\n\n### 为什么需要 FSDP？\n\n在实际多模态预训练中（如训练 7B+ 参数的 MLLM），单张 GPU 的显存无法容纳：\n- 模型参数（FP32: ~28GB for 7B params）\n- 梯度（同等大小）\n- 优化器状态（Adam: 2× 参数大小，即 ~56GB）\n- 总计约 **112GB**，远超单卡 80GB (A100)\n\n**FSDP (Fully Sharded Data Parallel)** 将上述三者都分片到多张 GPU 上，每张 GPU 只持有 1/N 的模型状态。\n\n### DDP vs FSDP 显存对比\n\n```\nDDP (Distributed Data Parallel):\n┌──────────────────┐  ┌──────────────────┐\n│     GPU 0        │  │     GPU 1        │\n│ 完整模型参数 (P) │  │ 完整模型参数 (P) │  ← 每张 GPU 持有完整副本\n│ 完整梯度     (G) │  │ 完整梯度     (G) │\n│ 完整优化器   (O) │  │ 完整优化器   (O) │\n│ 总计: P+G+O      │  │ 总计: P+G+O      │\n└──────────────────┘  └──────────────────┘\n显存占用 = N × (P + G + O)    ← 不随 GPU 数量减少！\n\nFSDP (Fully Sharded Data Parallel):\n┌──────────────────┐  ┌──────────────────┐\n│     GPU 0        │  │     GPU 1        │\n│ 参数分片 0  (P/N)│  │ 参数分片 1  (P/N)│  ← 每张 GPU 只有 1/N\n│ 梯度分片 0  (G/N)│  │ 梯度分片 1  (G/N)│\n│ 优化器分片 0(O/N)│  │ 优化器分片 1(O/N)│\n│ 总计: (P+G+O)/N  │  │ 总计: (P+G+O)/N  │\n└──────────────────┘  └──────────────────┘\n显存占用 ≈ (P + G + O) / N   ← GPU 越多，每卡越省！\n```\n\n### FSDP 训练的通信流程\n\n```\nForward Pass (逐层执行):\n  ① All-Gather:  收集该层的完整参数 (各 GPU 的分片 → 拼接为完整参数)\n  ② Compute:     用完整参数计算该层的前向传播\n  ③ Discard:     丢弃非本 GPU 负责的参数分片 (释放显存)\n\nBackward Pass (逐层执行, 反向顺序):\n  ① All-Gather:  再次收集该层完整参数\n  ② Compute:     计算该层的梯度\n  ③ Reduce-Scatter: 将梯度归约并分片到各 GPU\n  ④ Discard:     丢弃非本 GPU 负责的参数和梯度\n\nOptimizer Step:\n  - 每张 GPU 只更新自己负责的那一片参数 (无需通信)\n```\n\n### 三种分片策略\n\n| 策略 | 对应 DeepSpeed ZeRO | 分片内容 | 显存节省 | 通信量 |\n|------|---------------------|----------|----------|--------|\n| `FULL_SHARD` | ZeRO Stage 3 | 参数 + 梯度 + 优化器 | **最大** | 最大 |\n| `SHARD_GRAD_OP` | ZeRO Stage 2 | 梯度 + 优化器 | 中等 | 中等 |\n| `NO_SHARD` | DDP | 不分片 | 无 | 最小 |\n\n**选择建议**：\n- 模型能放进单 GPU → `NO_SHARD` (就是 DDP)\n- 模型参数能放下但优化器状态放不下 → `SHARD_GRAD_OP`\n- 模型参数也放不下 → `FULL_SHARD`"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ====================================================================\n# FSDP 核心配置代码详解\n#\n# 注意：以下代码展示 FSDP 的关键配置步骤，不能在 notebook 中直接运行。\n# FSDP 需要通过 torchrun 启动多个进程。\n# 完整可运行脚本: experiments/scripts/fsdp_pretrain.py\n# 运行方式: torchrun --nproc_per_node=2 experiments/scripts/fsdp_pretrain.py\n# ====================================================================\n\nprint('=' * 70)\nprint('FSDP 关键配置步骤（教学演示）')\nprint('=' * 70)\n\n# ---------- 步骤 1: 初始化分布式环境 ----------\nprint('''\n【步骤 1】初始化分布式环境\n\n  import torch.distributed as dist\n  dist.init_process_group(backend=\"nccl\")   # NCCL: GPU 通信最佳后端\n  local_rank = int(os.environ[\"LOCAL_RANK\"]) # torchrun 自动设置\n  torch.cuda.set_device(local_rank)\n''')\n\n# ---------- 步骤 2: 配置 FSDP ----------\nprint('''\n【步骤 2】配置 FSDP 策略\n\n  # 2a. 自动包装策略: 以 TransformerBlock 为单位进行分片\n  #     粒度太细(每个 Linear) → 通信过多\n  #     粒度太粗(整个 Encoder) → 显存节省不够\n  auto_wrap_policy = functools.partial(\n      transformer_auto_wrap_policy,\n      transformer_layer_cls={TransformerBlock},\n  )\n\n  # 2b. 混合精度: BF16 计算, 节省 ~50% 显存, 速度提升 ~2x\n  #     需要 Ampere+ GPU (A100, H100)\n  mp_policy = MixedPrecision(\n      param_dtype=torch.bfloat16,\n      reduce_dtype=torch.bfloat16,\n      buffer_dtype=torch.bfloat16,\n  )\n\n  # 2c. 分片策略\n  sharding = ShardingStrategy.FULL_SHARD   # ZeRO-3, 最省显存\n''')\n\n# ---------- 步骤 3: 包装模型 ----------\nprint('''\n【步骤 3】用 FSDP 包装模型\n\n  model = FSDP(\n      model,\n      sharding_strategy=sharding,\n      auto_wrap_policy=auto_wrap_policy,\n      mixed_precision=mp_policy,\n      backward_prefetch=BackwardPrefetch.BACKWARD_PRE,  # 预取下一层\n      device_id=local_rank,\n  )\n''')\n\n# ---------- 步骤 4: 数据加载 ----------\nprint('''\n【步骤 4】分布式数据加载\n\n  # 关键改动: shuffle=True → sampler=DistributedSampler(...)\n  sampler = DistributedSampler(dataset, shuffle=True)\n  loader = DataLoader(dataset, batch_size=256, sampler=sampler)\n\n  # 每个 epoch 开始时必须调用:\n  sampler.set_epoch(epoch)  # 确保每轮数据顺序不同\n''')\n\n# ---------- 步骤 5: 梯度裁剪 ----------\nprint('''\n【步骤 5】FSDP 下的梯度裁剪\n\n  # 错误 ✗  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n  # 正确 ✓  model.clip_grad_norm_(1.0)   # FSDP 提供的专用方法\n''')\n\n# ---------- 步骤 6: 保存 Checkpoint ----------\nprint('''\n【步骤 6】FSDP Checkpoint 保存\n\n  # 方式一: FULL_STATE_DICT (聚合到 rank 0 后保存)\n  save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n  with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, save_policy):\n      if dist.get_rank() == 0:\n          torch.save(model.state_dict(), \"checkpoint.pt\")\n\n  # 方式二: SHARDED_STATE_DICT (各 rank 保存自己的分片, 推荐大模型)\n  # import torch.distributed.checkpoint as dcp\n  # dcp.save(model.state_dict(), checkpoint_id=\"ckpt_dir/\")\n''')\n\n# ---------- 总结: 单 GPU → FSDP 的改动清单 ----------\nprint('=' * 70)\nprint('单 GPU → FSDP 的核心改动清单')\nprint('=' * 70)\nchanges = [\n    ('初始化', 'dist.init_process_group(\"nccl\")'),\n    ('模型包装', 'model = FSDP(model, ...)'),\n    ('数据加载', 'shuffle=True → sampler=DistributedSampler(...)'),\n    ('梯度裁剪', 'clip_grad_norm_ → model.clip_grad_norm_()'),\n    ('Checkpoint', '需要 FSDP.state_dict_type 上下文管理器'),\n    ('清理', '训练结束后调用 dist.destroy_process_group()'),\n]\nfor item, detail in changes:\n    print(f'  ✦ {item:12s}: {detail}')\n\nprint(f'\\n完整 FSDP 训练脚本: experiments/scripts/fsdp_pretrain.py')\nprint(f'运行: torchrun --nproc_per_node=NUM_GPUS experiments/scripts/fsdp_pretrain.py')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 实验结论\n\n### 关键发现\n\n1. **对比学习有效**: 即使在简化版模型 + 模板 Caption 的设置下，对比学习也能学到有意义的图文对齐\n2. **零样本迁移**: 模型从未见过显式的类别标签，仅通过图文对比学习就获得了分类能力\n3. **温度参数自适应**: `logit_scale` 在训练过程中自动调整，控制对比损失的\"锐利度\"\n\n### 与 CLIP 论文的对照\n\n| 维度 | CLIP 原版 | 本实验 |\n|------|-----------|--------|\n| 数据规模 | 4 亿互联网图文对 | 5 万 CIFAR-10 + 模板 Caption |\n| Image Encoder | ViT-L/14 (24层, 1024维) | 简化 ViT (6层, 256维) |\n| Text Encoder | 12层 GPT-2 风格 | 4层双向 Transformer |\n| Batch Size | 32,768 | 256 |\n| 训练规模 | 256×V100, 12天 | 单 GPU, 几分钟 |\n| ImageNet Zero-Shot | 76.2% | CIFAR-10 ~50-70% |\n\n### FSDP 学习要点\n\n1. **何时用 FSDP**: 模型参数 × 12 (FP32 Adam) > 单卡显存时\n2. **分片粒度**: 以 Transformer 层为单位包装，平衡通信与显存\n3. **与 DDP 的区别**: FSDP 在 forward/backward 时动态聚合/释放参数\n4. **Checkpoint**: 保存前需聚合分片参数，或使用分片 checkpoint\n5. **混合精度**: BF16 训练可节省约 50% 显存，速度提升约 2x\n\n### 后续探索方向\n\n- 使用真实图文数据集（Flickr30k、COCO Captions）替换模板 Caption\n- 增大模型规模，对比 FSDP 不同策略（FULL_SHARD vs SHARD_GRAD_OP）的显存和速度差异\n- 实现 Gradient Checkpointing (`torch.utils.checkpoint`) 进一步节省显存\n- 将 Vision Encoder 替换为预训练 CLIP ViT，只训练桥接层（模拟 LLaVA 的训练方式）\n- 在 FSDP 脚本中添加 Wandb/TensorBoard 日志，监控分布式训练指标"
  }
 ]
}
